{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692d2530-e99d-452e-9a29-f15e54613658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasso is a modification of linear regression, where the model is penalized for the sum of absolute values of the\\n   weights. Thus, the absolute values of weight will be (in general) reduced, and many will tend to be zeros.\\n   \\n   Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a\\n   penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the \\n   square. Ridge regression is also referred to as L2 Regularization.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Lasso is a modification of linear regression, where the model is penalized for the sum of absolute values of the\n",
    "   weights. Thus, the absolute values of weight will be (in general) reduced, and many will tend to be zeros.\n",
    "   \n",
    "   Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing a\n",
    "   penalty factor. However, while lasso regression takes the magnitude of the coefficients, ridge regression takes the \n",
    "   square. Ridge regression is also referred to as L2 Regularization.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c756e8f0-a13c-4ef2-9b49-64ce58aecfb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does\\n   not consider interesting to zero. This means that the model does some automatic feature selection to decide which \\n   features should and should not be included on its own.\\n   LASSO is a penalized regression method to improve OLS and Ridge regression. LASSO does shrinkage and variable \\n   selection simultaneously for better prediction and model interpretation.\\n   \\n   Disadvantage of LASSO:\\n   LASSO selects at most n variables before it saturates. LASSO can not do group selection.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does\n",
    "   not consider interesting to zero. This means that the model does some automatic feature selection to decide which \n",
    "   features should and should not be included on its own.\n",
    "   LASSO is a penalized regression method to improve OLS and Ridge regression. LASSO does shrinkage and variable \n",
    "   selection simultaneously for better prediction and model interpretation.\n",
    "   \n",
    "   Disadvantage of LASSO:\n",
    "   LASSO selects at most n variables before it saturates. LASSO can not do group selection.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bea6751-eb93-4045-bcd8-a1ac0147cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The estimated coefficients target the same targets, and both have some estimation error (which, if squared, can be\\n   decomposed into bias and variance), so in this sense their interpretation is the same. Now of course the methods are\\n   not the same, so you get different estimated coefficient values.\\n   Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of \\n   coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can \\n   become zero and eliminated from the model.\\n   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''The estimated coefficients target the same targets, and both have some estimation error (which, if squared, can be\n",
    "   decomposed into bias and variance), so in this sense their interpretation is the same. Now of course the methods are\n",
    "   not the same, so you get different estimated coefficient values.\n",
    "   Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of \n",
    "   coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can \n",
    "   become zero and eliminated from the model.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "497b6764-5580-4f4a-ba53-7253008d9c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge\\n   regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a\\n   central point, like the mean.\\n   \\n   The default value of regularization parameter in Lasso regression (given by α) is 1. With this, out of 30 \\n   features in cancer data-set, only 4 features are used (non zero value of the coefficient). Both training and test \\n   score (with only 4 features) are low; conclude that the model is under-fitting the cancer data-set.\\n   \\n   The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength \\n   (sklearn documentation). Solver is the algorithm to use in the optimization problem. The choices are\\n   {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'.\\n   \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge\n",
    "   regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a\n",
    "   central point, like the mean.\n",
    "   \n",
    "   The default value of regularization parameter in Lasso regression (given by α) is 1. With this, out of 30 \n",
    "   features in cancer data-set, only 4 features are used (non zero value of the coefficient). Both training and test \n",
    "   score (with only 4 features) are low; conclude that the model is under-fitting the cancer data-set.\n",
    "   \n",
    "   The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength \n",
    "   (sklearn documentation). Solver is the algorithm to use in the optimization problem. The choices are\n",
    "   {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f92c69cd-5df9-4390-bf2f-1abc3b23abf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression\\n   models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby\\n   selecting the number of basis functions effectively.\\n   One example of how nonlinear regression can be used is to predict population growth over time. A scatterplot of \\n   changing population data over time shows that there seems to be a relationship between time and population growth,\\n   but that it is a nonlinear relationship, requiring the use of a nonlinear regression model.\\n   '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression\n",
    "   models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby\n",
    "   selecting the number of basis functions effectively.\n",
    "   One example of how nonlinear regression can be used is to predict population growth over time. A scatterplot of \n",
    "   changing population data over time shows that there seems to be a relationship between time and population growth,\n",
    "   but that it is a nonlinear relationship, requiring the use of a nonlinear regression model.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82db3ec7-2dcb-44c1-b84d-63cc377620d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lasso regression takes the magnitude of the coefficients, ridge regression takes the square. \\n   Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing\\n   a penalty factor. Ridge regression is also referred to as L2 Regularization.\\n   \\n   Lasso will eliminate many features, and reduce overfitting in your linear model.\\n   Ridge will reduce the impact of features that are not important in predicting your y values.\\n   \\n    Elastic Net combines feature elimination from Lasso and\\n    feature coefficient reduction from the Ridge model to improve your model's predictions.\\n    \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Lasso regression takes the magnitude of the coefficients, ridge regression takes the square. \n",
    "   Similar to the lasso regression, ridge regression puts a similar constraint on the coefficients by introducing\n",
    "   a penalty factor. Ridge regression is also referred to as L2 Regularization.\n",
    "   \n",
    "   Lasso will eliminate many features, and reduce overfitting in your linear model.\n",
    "   Ridge will reduce the impact of features that are not important in predicting your y values.\n",
    "   \n",
    "    Elastic Net combines feature elimination from Lasso and\n",
    "    feature coefficient reduction from the Ridge model to improve your model's predictions.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70038947-07d7-4b0d-8831-b01d0fcd4d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lasso Regression\\n   Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator\\n   (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather\\n   than the L2 norm as a measure of complexity.\\n   \\n   Lasso regression is a linear regression technique with L1 prior as a regularize. The idea is to reduce the \\n   multicollinearity by regularization by reducing the coefficients of the feature that are multicollinear.\\n   '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''Lasso Regression\n",
    "   Another Tolerant Method for dealing with multicollinearity known as Least Absolute Shrinkage and Selection Operator\n",
    "   (LASSO) regression, solves the same constrained optimization problem as ridge regression, but uses the L1 norm rather\n",
    "   than the L2 norm as a measure of complexity.\n",
    "   \n",
    "   Lasso regression is a linear regression technique with L1 prior as a regularize. The idea is to reduce the \n",
    "   multicollinearity by regularization by reducing the coefficients of the feature that are multicollinear.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3f33e4-2a98-4201-8655-4cf25278c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "'''When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit:\n",
    "   If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your\n",
    "   model won't learn enough about the training data to make useful predictions.\n",
    "   The value of lambda will be chosen by cross-validation. The plot shows cross-validated mean squared error. As lambda\n",
    "   decreases, the mean squared error decreases. Ridge includes all the variables in the model and the value of lambda selected is indicated by the vertical lines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
